{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkXZIr9QFB9G"
   },
   "source": [
    "**Install Apache Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IfH0l1VKDn2Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install Java8\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# download spark3.0.1\n",
    "!wget -q http://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
    "# unzip it\n",
    "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
    "# install findspark \n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vG1hsVGGwKq"
   },
   "source": [
    "**Set Environment Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "q4vR_4ZzG_Eq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h79I7DWHHhuX"
   },
   "source": [
    "**Testing spark installation and version**\n",
    "- Spark should be version 3.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IqrtHKafHMVJ",
    "outputId": "fe00eac7-f73d-4bec-bcb5-ea9018bc29ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|world|\n",
      "|world|\n",
      "|world|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "# Test the spark\n",
    "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
    "df.show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1WI4744HtFp"
   },
   "source": [
    "We can also check the pyspark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XzfBxURmHm9D",
    "outputId": "03403753-9542-4219-d42c-4e2785723c50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sopDjDH-ICTC"
   },
   "source": [
    "We have our environment setup. Now let's get into the basics. We first need to import pyspark but we already did this in the previous step so we will proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZFw3TSUIc5j"
   },
   "source": [
    "## Spark Basics\n",
    "\n",
    "### Creating RDDs\n",
    "After importing pyspark we can create a SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6KfqNq7GHzAx"
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPh_xs7zJA7-"
   },
   "source": [
    "**Example 1: creating RDDs from an array of numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3GQf1M5I2UP",
    "outputId": "30c40c4d-f9e6-4717-cc4a-298c3459c6a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "data = [num for num in range(1,10)]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XoXCu1koJKPB"
   },
   "outputs": [],
   "source": [
    "myRDD = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NUHNkX63Jnbk",
    "outputId": "87dc3a45-c7f0-45a4-ceb8-d8b69b7fb131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(myRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVemFoTBJpVW",
    "outputId": "ebf4c005-bd76-4005-ce68-929f9e212880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(myRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPKy3GP1J0yH"
   },
   "source": [
    "**Example 2: creating RDDs from key value pairs (tuples)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LuabLzh1Ju_v",
    "outputId": "fa524163-61d6-455a-d2ae-486ecb395260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7), ('a', 2), ('b', 2), ('b', 4), ('c', 1), ('c', 2), ('c', 3), ('c', 4)]\n"
     ]
    }
   ],
   "source": [
    "kv = [('a',7), ('a', 2), ('b', 2), ('b',4), ('c',1), ('c',2), ('c',3), ('c',4)]\n",
    "print(kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99DJG6woJ9X7",
    "outputId": "224c56de-9aea-4b81-b832-e0690ccabdd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7), ('a', 2), ('b', 2), ('b', 4), ('c', 1), ('c', 2), ('c', 3), ('c', 4)]\n"
     ]
    }
   ],
   "source": [
    "rdd2 = sc.parallelize(kv)\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5KqIdPUrKAdi",
    "outputId": "ad653ac6-6797-4d11-cb8d-c8215aaedcbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 6), ('c', 10), ('a', 9)]\n"
     ]
    }
   ],
   "source": [
    "rdd3 = rdd2.reduceByKey(lambda x, y: x+y)\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AiJuhNV_KDUX",
    "outputId": "c64934b4-c05b-4fe6-bd0a-d9d97b77abe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', <pyspark.resultiterable.ResultIterable object at 0x7f37f79d2828>), ('c', <pyspark.resultiterable.ResultIterable object at 0x7f37f79d2588>), ('a', <pyspark.resultiterable.ResultIterable object at 0x7f37f79d24e0>)]\n"
     ]
    }
   ],
   "source": [
    "rdd4 = rdd2.groupByKey()\n",
    "print(rdd4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOP0cshHKGeK",
    "outputId": "57a0de27-2493-4099-f301-307faf3c78cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', [2, 4]), ('c', [1, 2, 3, 4]), ('a', [7, 2])]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.map(lambda x: (x[0], list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLGG3_tWKKvx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Spark Basics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
