{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Basics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkXZIr9QFB9G"
      },
      "source": [
        "**Install Apache Spark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfH0l1VKDn2Z"
      },
      "source": [
        "# install Java8\r\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "# download spark3.0.1\r\n",
        "!wget -q http://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\r\n",
        "# unzip it\r\n",
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\r\n",
        "# install findspark \r\n",
        "!pip install -q findspark"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vG1hsVGGwKq"
      },
      "source": [
        "**Set Environment Variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4vR_4ZzG_Eq"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h79I7DWHHhuX"
      },
      "source": [
        "**Testing spark installation and version**\r\n",
        "- Spark should be version 3.0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqrtHKafHMVJ",
        "outputId": "fe00eac7-f73d-4bec-bcb5-ea9018bc29ba"
      },
      "source": [
        "import findspark\r\n",
        "findspark.init()\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\r\n",
        "# Test the spark\r\n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\r\n",
        "df.show(3, False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1WI4744HtFp"
      },
      "source": [
        "We can also check the pyspark version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzfBxURmHm9D",
        "outputId": "03403753-9542-4219-d42c-4e2785723c50"
      },
      "source": [
        "import pyspark\r\n",
        "print(pyspark.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sopDjDH-ICTC"
      },
      "source": [
        "We have our environment setup. Now let's get into the basics. We first need to import pyspark but we already did this in the previous step so we will proceed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZFw3TSUIc5j"
      },
      "source": [
        "## Spark Basics\r\n",
        "\r\n",
        "### Creating RDDs\r\n",
        "After importing pyspark we can create a SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KfqNq7GHzAx"
      },
      "source": [
        "sc = pyspark.SparkContext.getOrCreate()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPh_xs7zJA7-"
      },
      "source": [
        "**Example 1: creating RDDs from an array of numbers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3GQf1M5I2UP",
        "outputId": "30c40c4d-f9e6-4717-cc4a-298c3459c6a1"
      },
      "source": [
        "data = [num for num in range(1,10)]\r\n",
        "print(data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoXCu1koJKPB"
      },
      "source": [
        "myRDD = sc.parallelize(data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUHNkX63Jnbk",
        "outputId": "87dc3a45-c7f0-45a4-ceb8-d8b69b7fb131"
      },
      "source": [
        "print(myRDD.collect())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVemFoTBJpVW",
        "outputId": "ebf4c005-bd76-4005-ce68-929f9e212880"
      },
      "source": [
        "print(myRDD.count())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPKy3GP1J0yH"
      },
      "source": [
        "**Example 2: creating RDDs from key value pairs (tuples)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuabLzh1Ju_v",
        "outputId": "fa524163-61d6-455a-d2ae-486ecb395260"
      },
      "source": [
        "kv = [('a',7), ('a', 2), ('b', 2), ('b',4), ('c',1), ('c',2), ('c',3), ('c',4)]\r\n",
        "print(kv)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('a', 7), ('a', 2), ('b', 2), ('b', 4), ('c', 1), ('c', 2), ('c', 3), ('c', 4)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99DJG6woJ9X7",
        "outputId": "224c56de-9aea-4b81-b832-e0690ccabdd2"
      },
      "source": [
        "rdd2 = sc.parallelize(kv)\r\n",
        "print(rdd2.collect())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('a', 7), ('a', 2), ('b', 2), ('b', 4), ('c', 1), ('c', 2), ('c', 3), ('c', 4)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KqIdPUrKAdi",
        "outputId": "ad653ac6-6797-4d11-cb8d-c8215aaedcbf"
      },
      "source": [
        "rdd3 = rdd2.reduceByKey(lambda x, y: x+y)\r\n",
        "print(rdd3.collect())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('b', 6), ('c', 10), ('a', 9)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiJuhNV_KDUX",
        "outputId": "c64934b4-c05b-4fe6-bd0a-d9d97b77abe4"
      },
      "source": [
        "rdd4 = rdd2.groupByKey()\r\n",
        "print(rdd4.collect())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('b', <pyspark.resultiterable.ResultIterable object at 0x7f37f79d2828>), ('c', <pyspark.resultiterable.ResultIterable object at 0x7f37f79d2588>), ('a', <pyspark.resultiterable.ResultIterable object at 0x7f37f79d24e0>)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOP0cshHKGeK",
        "outputId": "57a0de27-2493-4099-f301-307faf3c78cc"
      },
      "source": [
        "rdd4.map(lambda x: (x[0], list(x[1]))).collect()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', [2, 4]), ('c', [1, 2, 3, 4]), ('a', [7, 2])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLGG3_tWKKvx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}